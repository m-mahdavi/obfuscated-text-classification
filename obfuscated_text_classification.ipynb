{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obfuscated Text Classification\n",
    "### Mohammad Mahdavi,  1 October 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the necessary modules\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib\n",
    "import IPython.display\n",
    "\n",
    "import sklearn.tree\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "import sklearn.neural_network\n",
    "import sklearn.model_selection\n",
    "import sklearn.feature_extraction\n",
    "\n",
    "import keras.utils\n",
    "import keras.models\n",
    "import keras.layers\n",
    "import keras.optimizers\n",
    "import keras.preprocessing\n",
    "import keras.preprocessing.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### Problem and Challenges\n",
    "We have a multiclass text classification task. However, there is a big challenge here: The input text is obfuscated. In fact, our input text is binary, just like image data. That is why we cannot leverage typical feature extraction approaches to represent the input text. In particular, we cannot tokenize, stem, normalize, etc. the input text.\n",
    "\n",
    "\n",
    "### Approach Overview\n",
    "Intuitively, we should design deep neural networks, such as CNN and LSTM, that can effectively and efficently featurize and classify these kinds of obfuscated input data. \n",
    "\n",
    "To principally address this problem, we conduct the following steps:\n",
    "1. **Data exploration.** We first explore the data to understand the characteristics of our dataset.\n",
    "2. **Traditional text classification.** We then build a traditional text classification pipeline to see what we can achieve without applying any complex deep learning models. \n",
    "3. **Deep learning-based text classification**. Based on the gained insigts from the first two steps, we next train a deep neural network to classify the obfuscated texts effectively and efficiently.\n",
    "4. **Evaluating the trained model.** We then evaluate the trained model on an unseen validation set, splitted from the labeled training data.\n",
    "5. **Conclusion.** We finally conclude our task with summarizing the insights and listing the limitations and the future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "Let us first load and explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Rows</th>\n",
       "      <th>Min Length</th>\n",
       "      <th>Max Length</th>\n",
       "      <th>Unique Characters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train</td>\n",
       "      <td>32513</td>\n",
       "      <td>168</td>\n",
       "      <td>452</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test</td>\n",
       "      <td>3000</td>\n",
       "      <td>72</td>\n",
       "      <td>448</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Data   Rows Min Length Max Length Unique Characters\n",
       "0  Train  32513        168        452                26\n",
       "1   Test   3000         72        448                26"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f10fc63f1d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_train = open(\"xtrain_obfuscated.txt\", \"r\").read().splitlines()\n",
    "text_test = open(\"xtest_obfuscated.txt\", \"r\").read().splitlines()\n",
    "labels_train = open(\"ytrain.txt\", \"r\").read().splitlines()\n",
    "\n",
    "metadata = pandas.DataFrame(columns=[\"Data\", \"Rows\", \"Min Length\", \"Max Length\", \"Unique Characters\"])\n",
    "metadata = metadata.append({\"Data\": \"Train\", \"Rows\": len(text_train), \n",
    "                            \"Min Length\": len(min(text_train, key=len)), \n",
    "                            \"Max Length\": len(max(text_train, key=len)), \n",
    "                            \"Unique Characters\": len(list(set(\"\".join(text_train))))}, ignore_index=True)\n",
    "metadata = metadata.append({\"Data\": \"Test\", \"Rows\": len(text_test), \n",
    "                            \"Min Length\": len(min(text_test, key=len)), \n",
    "                            \"Max Length\": len(max(text_test, key=len)), \n",
    "                            \"Unique Characters\": len(list(set(\"\".join(text_test))))}, ignore_index=True)\n",
    "IPython.display.display(metadata)\n",
    "pandas.DataFrame({\"Labels\": labels_train})[\"Labels\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "1. The obfuscated texts consist of 26 English characters. There is no spaces or other delimiters to tokenize the texts. Therefore, we cannot use any word-level representation and we must go for **character-level text representations**.\n",
    "2. The classes are not significantly imblanced, which shows we can use **accuracy** as the evaluation measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Traditional Text Classification\n",
    "As mentioned, intuitively, it makes more sense to train deep neural networks for this obfuscated text classification task. However, it is always nice to start with simpler approaches to gain some insights about their performances.\n",
    "Here, we first design a simple text classification pipleline with typical character-level TF-IDF representaion and traditional classifiers. We then experiment the effect of the most important design decisions on the cross-validated performance. Based on the achieved insights, we will design a deep learning-based approach in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model = DTC\n",
      "Model = MLP\n",
      "Model = GBC\n",
      "N-Gram = 2\n",
      "N-Gram = 3\n",
      "N-Gram = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>N-Gram</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DTC</td>\n",
       "      <td>1</td>\n",
       "      <td>0.214653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP</td>\n",
       "      <td>1</td>\n",
       "      <td>0.401778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GBC</td>\n",
       "      <td>1</td>\n",
       "      <td>0.375481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DTC</td>\n",
       "      <td>2</td>\n",
       "      <td>0.377172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DTC</td>\n",
       "      <td>3</td>\n",
       "      <td>0.479777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model N-Gram  Accuracy\n",
       "0   DTC      1  0.214653\n",
       "1   MLP      1  0.401778\n",
       "2   GBC      1  0.375481\n",
       "3   DTC      2  0.377172\n",
       "4   DTC      3  0.479777"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "def traditional_text_classification(model=\"DTC\", n_gram=1):\n",
    "    \"\"\"\n",
    "    This function builds a simple text classification pipeline based on the input parameters.\n",
    "    \"\"\"\n",
    "    # Featurizing the data with charater-level TF-IDF representation\n",
    "    tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1, n_gram), analyzer=\"char\")\n",
    "    tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(text_train)\n",
    "    x_train = tfidf_vectorizer_vectors\n",
    "    y_train = labels_train\n",
    "\n",
    "    # Training and testing a classifier\n",
    "    if model == \"DTC\":\n",
    "        parameters_grid = {\"criterion\": [\"gini\", \"entropy\"]}\n",
    "        clf = sklearn.model_selection.GridSearchCV(sklearn.tree.DecisionTreeClassifier(), \n",
    "                                                   parameters_grid, scoring=\"accuracy\", cv=5, n_jobs=-1)\n",
    "    elif model == \"MLP\":\n",
    "        parameters_grid = {\"hidden_layer_sizes\": [50, 100, 200], \"max_iter\": [300]}\n",
    "        clf = sklearn.model_selection.GridSearchCV(sklearn.neural_network.MLPClassifier(), \n",
    "                                                   parameters_grid, scoring=\"accuracy\", cv=5, n_jobs=-1)\n",
    "    elif model == \"GBC\":\n",
    "        parameters_grid = {\"n_estimators\": [50, 100, 200]}\n",
    "        clf = sklearn.model_selection.GridSearchCV(sklearn.ensemble.GradientBoostingClassifier(), \n",
    "                                                   parameters_grid, scoring=\"accuracy\", cv=5, n_jobs=-1)\n",
    "    clf.fit(x_train, y_train)  \n",
    "    return {\"Model\": model, \"N-Gram\": n_gram, \"Accuracy\": max(clf.cv_results_[\"mean_test_score\"])}\n",
    "\n",
    "\n",
    "results = pandas.DataFrame(columns=[\"Model\", \"N-Gram\", \"Accuracy\"])\n",
    "\n",
    "for model in [\"DTC\", \"MLP\", \"GBC\"]:\n",
    "    print(\"Model =\", model)\n",
    "    result = traditional_text_classification(model=model)\n",
    "    results = results.append(result, ignore_index=True)\n",
    "    \n",
    "for n_gram in [2, 3, 4]:\n",
    "    print(\"N-Gram =\", n_gram)\n",
    "    result = traditional_text_classification(n_gram=n_gram)\n",
    "    results = results.append(result, ignore_index=True)\n",
    "    \n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "1. As expected, the **traditional feature representations** and classifiers **cannot achieve high accuracy** due to the nature of obfuscated input text data. We need to use deep learning-based approaches to achieve higher accuracy. \n",
    "2. Among different traditional classifiers that we tried, MLP achieves the highest accuracy. This observation also suggests to go for **deep neural networks** as the neural network-based models seem to be more suitable for this task rather than tree-based or ensemble-based models, such as decision tree and gradient boosting. \n",
    "3. Increasing N in the n-gram character-level representation clearly improves the accuracy. This observation shows that **recurrent deep neural networks**, such as LSTM, that are designed to **learn language modeling** could be promising for this task. That is why we will train an LSTM-based model in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deep Learning-Based Text Classification \n",
    "We can now train a deep neural network based on the insights we have gained so far. Therefore, we train a deep neural network including LSTM and CNN layers to learn the language modele of characters. We also split the data into train and validation sets to evaluate our model at the end on an unseen validation set. Note that there are a lot of hyperparameters in our designed network that can affect the performance of our model. We set them by try and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 452, 50)           1350      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 452, 50)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 452, 50)           200       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 448, 64)           16064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 112, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                23000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               10200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 12)                2412      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 54,226\n",
      "Trainable params: 53,626\n",
      "Non-trainable params: 600\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "84/84 [==============================] - 23s 274ms/step - loss: 0.5776 - accuracy: 0.1474 - val_loss: 0.3831 - val_accuracy: 0.1265\n",
      "Epoch 2/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.2818 - accuracy: 0.2697 - val_loss: 0.2783 - val_accuracy: 0.1836\n",
      "Epoch 3/500\n",
      "84/84 [==============================] - 21s 255ms/step - loss: 0.2235 - accuracy: 0.3928 - val_loss: 0.3064 - val_accuracy: 0.1638\n",
      "Epoch 4/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.1917 - accuracy: 0.5077 - val_loss: 0.3855 - val_accuracy: 0.1498\n",
      "Epoch 5/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.1711 - accuracy: 0.5630 - val_loss: 0.4027 - val_accuracy: 0.2270\n",
      "Epoch 6/500\n",
      "84/84 [==============================] - 21s 254ms/step - loss: 0.1580 - accuracy: 0.5978 - val_loss: 0.4055 - val_accuracy: 0.2651\n",
      "Epoch 7/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.1487 - accuracy: 0.6212 - val_loss: 0.2574 - val_accuracy: 0.4321\n",
      "Epoch 8/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.1419 - accuracy: 0.6447 - val_loss: 0.2225 - val_accuracy: 0.5104\n",
      "Epoch 9/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.1371 - accuracy: 0.6564 - val_loss: 0.1589 - val_accuracy: 0.6196\n",
      "Epoch 10/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.1313 - accuracy: 0.6736 - val_loss: 0.1417 - val_accuracy: 0.6530\n",
      "Epoch 11/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.1274 - accuracy: 0.6843 - val_loss: 0.1585 - val_accuracy: 0.6584\n",
      "Epoch 12/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.1242 - accuracy: 0.6960 - val_loss: 0.1380 - val_accuracy: 0.6578\n",
      "Epoch 13/500\n",
      "84/84 [==============================] - 22s 268ms/step - loss: 0.1216 - accuracy: 0.7020 - val_loss: 0.1400 - val_accuracy: 0.6647\n",
      "Epoch 14/500\n",
      "84/84 [==============================] - 21s 254ms/step - loss: 0.1182 - accuracy: 0.7100 - val_loss: 0.1511 - val_accuracy: 0.6340\n",
      "Epoch 15/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.1145 - accuracy: 0.7218 - val_loss: 0.1521 - val_accuracy: 0.6561\n",
      "Epoch 16/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.1130 - accuracy: 0.7266 - val_loss: 0.1488 - val_accuracy: 0.6559\n",
      "Epoch 17/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.1108 - accuracy: 0.7334 - val_loss: 0.1344 - val_accuracy: 0.6897\n",
      "Epoch 18/500\n",
      "84/84 [==============================] - 21s 256ms/step - loss: 0.1087 - accuracy: 0.7397 - val_loss: 0.1489 - val_accuracy: 0.6557\n",
      "Epoch 19/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.1072 - accuracy: 0.7421 - val_loss: 0.1385 - val_accuracy: 0.6757\n",
      "Epoch 20/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.1058 - accuracy: 0.7466 - val_loss: 0.1486 - val_accuracy: 0.6619\n",
      "Epoch 21/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.1036 - accuracy: 0.7523 - val_loss: 0.1304 - val_accuracy: 0.6988\n",
      "Epoch 22/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.1014 - accuracy: 0.7585 - val_loss: 0.1434 - val_accuracy: 0.6711\n",
      "Epoch 23/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.1003 - accuracy: 0.7573 - val_loss: 0.1383 - val_accuracy: 0.6849\n",
      "Epoch 24/500\n",
      "84/84 [==============================] - 22s 256ms/step - loss: 0.0993 - accuracy: 0.7649 - val_loss: 0.1304 - val_accuracy: 0.6918\n",
      "Epoch 25/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0973 - accuracy: 0.7685 - val_loss: 0.1145 - val_accuracy: 0.7313\n",
      "Epoch 26/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0969 - accuracy: 0.7695 - val_loss: 0.1335 - val_accuracy: 0.6993\n",
      "Epoch 27/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0949 - accuracy: 0.7744 - val_loss: 0.1429 - val_accuracy: 0.6999\n",
      "Epoch 28/500\n",
      "84/84 [==============================] - 22s 256ms/step - loss: 0.0945 - accuracy: 0.7780 - val_loss: 0.1278 - val_accuracy: 0.7188\n",
      "Epoch 29/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0930 - accuracy: 0.7792 - val_loss: 0.1180 - val_accuracy: 0.7188\n",
      "Epoch 30/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0924 - accuracy: 0.7797 - val_loss: 0.1122 - val_accuracy: 0.7366\n",
      "Epoch 31/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0912 - accuracy: 0.7882 - val_loss: 0.1329 - val_accuracy: 0.7068\n",
      "Epoch 32/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.0906 - accuracy: 0.7897 - val_loss: 0.1228 - val_accuracy: 0.7234\n",
      "Epoch 33/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0890 - accuracy: 0.7899 - val_loss: 0.1412 - val_accuracy: 0.6995\n",
      "Epoch 34/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0875 - accuracy: 0.7937 - val_loss: 0.1273 - val_accuracy: 0.6988\n",
      "Epoch 35/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0875 - accuracy: 0.7946 - val_loss: 0.1345 - val_accuracy: 0.7072\n",
      "Epoch 36/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0871 - accuracy: 0.7954 - val_loss: 0.1383 - val_accuracy: 0.6855\n",
      "Epoch 37/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0855 - accuracy: 0.8024 - val_loss: 0.1224 - val_accuracy: 0.7311\n",
      "Epoch 38/500\n",
      "84/84 [==============================] - 21s 255ms/step - loss: 0.0851 - accuracy: 0.8018 - val_loss: 0.1132 - val_accuracy: 0.7378\n",
      "Epoch 39/500\n",
      "84/84 [==============================] - 21s 256ms/step - loss: 0.0844 - accuracy: 0.8045 - val_loss: 0.1132 - val_accuracy: 0.7482\n",
      "Epoch 40/500\n",
      "84/84 [==============================] - 21s 255ms/step - loss: 0.0833 - accuracy: 0.8056 - val_loss: 0.1082 - val_accuracy: 0.7514\n",
      "Epoch 41/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0834 - accuracy: 0.8033 - val_loss: 0.1146 - val_accuracy: 0.7426\n",
      "Epoch 42/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0839 - accuracy: 0.8069 - val_loss: 0.1106 - val_accuracy: 0.7478\n",
      "Epoch 43/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0816 - accuracy: 0.8105 - val_loss: 0.1165 - val_accuracy: 0.7332\n",
      "Epoch 44/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0807 - accuracy: 0.8116 - val_loss: 0.1024 - val_accuracy: 0.7626\n",
      "Epoch 45/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0804 - accuracy: 0.8107 - val_loss: 0.1165 - val_accuracy: 0.7324\n",
      "Epoch 46/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.0808 - accuracy: 0.8131 - val_loss: 0.1147 - val_accuracy: 0.7445\n",
      "Epoch 47/500\n",
      "84/84 [==============================] - 21s 256ms/step - loss: 0.0782 - accuracy: 0.8209 - val_loss: 0.1439 - val_accuracy: 0.6805\n",
      "Epoch 48/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0797 - accuracy: 0.8144 - val_loss: 0.1184 - val_accuracy: 0.7495\n",
      "Epoch 49/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0792 - accuracy: 0.8179 - val_loss: 0.1308 - val_accuracy: 0.7092\n",
      "Epoch 50/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.0779 - accuracy: 0.8195 - val_loss: 0.0999 - val_accuracy: 0.7714\n",
      "Epoch 51/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0774 - accuracy: 0.8218 - val_loss: 0.1190 - val_accuracy: 0.7413\n",
      "Epoch 52/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0776 - accuracy: 0.8223 - val_loss: 0.1140 - val_accuracy: 0.7341\n",
      "Epoch 53/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0759 - accuracy: 0.8246 - val_loss: 0.1201 - val_accuracy: 0.7357\n",
      "Epoch 54/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0754 - accuracy: 0.8268 - val_loss: 0.1145 - val_accuracy: 0.7393\n",
      "Epoch 55/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0748 - accuracy: 0.8277 - val_loss: 0.1185 - val_accuracy: 0.7370\n",
      "Epoch 56/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0747 - accuracy: 0.8292 - val_loss: 0.1063 - val_accuracy: 0.7637\n",
      "Epoch 57/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0747 - accuracy: 0.8283 - val_loss: 0.1295 - val_accuracy: 0.7228\n",
      "Epoch 58/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.0734 - accuracy: 0.8320 - val_loss: 0.1138 - val_accuracy: 0.7443\n",
      "Epoch 59/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0738 - accuracy: 0.8322 - val_loss: 0.1069 - val_accuracy: 0.7707\n",
      "Epoch 60/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0728 - accuracy: 0.8340 - val_loss: 0.1131 - val_accuracy: 0.7480\n",
      "Epoch 61/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0718 - accuracy: 0.8356 - val_loss: 0.1068 - val_accuracy: 0.7561\n",
      "Epoch 62/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0726 - accuracy: 0.8343 - val_loss: 0.1300 - val_accuracy: 0.7309\n",
      "Epoch 63/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0714 - accuracy: 0.8376 - val_loss: 0.1165 - val_accuracy: 0.7395\n",
      "Epoch 64/500\n",
      "84/84 [==============================] - 22s 267ms/step - loss: 0.0702 - accuracy: 0.8389 - val_loss: 0.1224 - val_accuracy: 0.7355\n",
      "Epoch 65/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0708 - accuracy: 0.8394 - val_loss: 0.1103 - val_accuracy: 0.7622\n",
      "Epoch 66/500\n",
      "84/84 [==============================] - 21s 254ms/step - loss: 0.0713 - accuracy: 0.8357 - val_loss: 0.1162 - val_accuracy: 0.7463\n",
      "Epoch 67/500\n",
      "84/84 [==============================] - 23s 268ms/step - loss: 0.0692 - accuracy: 0.8412 - val_loss: 0.1139 - val_accuracy: 0.7603\n",
      "Epoch 68/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0700 - accuracy: 0.8404 - val_loss: 0.1118 - val_accuracy: 0.7559\n",
      "Epoch 69/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0692 - accuracy: 0.8416 - val_loss: 0.1189 - val_accuracy: 0.7395\n",
      "Epoch 70/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0688 - accuracy: 0.8433 - val_loss: 0.1157 - val_accuracy: 0.7539\n",
      "Epoch 71/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0703 - accuracy: 0.8419 - val_loss: 0.1329 - val_accuracy: 0.7263\n",
      "Epoch 72/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0696 - accuracy: 0.8439 - val_loss: 0.1087 - val_accuracy: 0.7661\n",
      "Epoch 73/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0682 - accuracy: 0.8449 - val_loss: 0.1092 - val_accuracy: 0.7628\n",
      "Epoch 74/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.0670 - accuracy: 0.8468 - val_loss: 0.1081 - val_accuracy: 0.7691\n",
      "Epoch 75/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0668 - accuracy: 0.8481 - val_loss: 0.1112 - val_accuracy: 0.7614\n",
      "Epoch 76/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0675 - accuracy: 0.8490 - val_loss: 0.1046 - val_accuracy: 0.7645\n",
      "Epoch 77/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0660 - accuracy: 0.8528 - val_loss: 0.1054 - val_accuracy: 0.7807\n",
      "Epoch 78/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0653 - accuracy: 0.8518 - val_loss: 0.1180 - val_accuracy: 0.7466\n",
      "Epoch 79/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0668 - accuracy: 0.8478 - val_loss: 0.1121 - val_accuracy: 0.7589\n",
      "Epoch 80/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0657 - accuracy: 0.8518 - val_loss: 0.1132 - val_accuracy: 0.7599\n",
      "Epoch 81/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0653 - accuracy: 0.8527 - val_loss: 0.1074 - val_accuracy: 0.7693\n",
      "Epoch 82/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0645 - accuracy: 0.8532 - val_loss: 0.1134 - val_accuracy: 0.7707\n",
      "Epoch 83/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0643 - accuracy: 0.8546 - val_loss: 0.1100 - val_accuracy: 0.7687\n",
      "Epoch 84/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0643 - accuracy: 0.8533 - val_loss: 0.1105 - val_accuracy: 0.7626\n",
      "Epoch 85/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0650 - accuracy: 0.8528 - val_loss: 0.1112 - val_accuracy: 0.7589\n",
      "Epoch 86/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0639 - accuracy: 0.8550 - val_loss: 0.1159 - val_accuracy: 0.7586\n",
      "Epoch 87/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0637 - accuracy: 0.8569 - val_loss: 0.1248 - val_accuracy: 0.7276\n",
      "Epoch 88/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0643 - accuracy: 0.8550 - val_loss: 0.1096 - val_accuracy: 0.7614\n",
      "Epoch 89/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0618 - accuracy: 0.8619 - val_loss: 0.1145 - val_accuracy: 0.7593\n",
      "Epoch 90/500\n",
      "84/84 [==============================] - 23s 269ms/step - loss: 0.0630 - accuracy: 0.8576 - val_loss: 0.1125 - val_accuracy: 0.7649\n",
      "Epoch 91/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.0619 - accuracy: 0.8617 - val_loss: 0.1084 - val_accuracy: 0.7759\n",
      "Epoch 92/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0620 - accuracy: 0.8616 - val_loss: 0.1161 - val_accuracy: 0.7482\n",
      "Epoch 93/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0626 - accuracy: 0.8600 - val_loss: 0.1399 - val_accuracy: 0.7351\n",
      "Epoch 94/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0616 - accuracy: 0.8600 - val_loss: 0.1056 - val_accuracy: 0.7803\n",
      "Epoch 95/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0619 - accuracy: 0.8630 - val_loss: 0.1110 - val_accuracy: 0.7657\n",
      "Epoch 96/500\n",
      "84/84 [==============================] - 22s 256ms/step - loss: 0.0613 - accuracy: 0.8644 - val_loss: 0.1078 - val_accuracy: 0.7793\n",
      "Epoch 97/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0602 - accuracy: 0.8644 - val_loss: 0.1094 - val_accuracy: 0.7714\n",
      "Epoch 98/500\n",
      "84/84 [==============================] - 21s 256ms/step - loss: 0.0601 - accuracy: 0.8681 - val_loss: 0.1148 - val_accuracy: 0.7495\n",
      "Epoch 99/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0600 - accuracy: 0.8641 - val_loss: 0.1043 - val_accuracy: 0.7743\n",
      "Epoch 100/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0594 - accuracy: 0.8665 - val_loss: 0.1106 - val_accuracy: 0.7695\n",
      "Epoch 101/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0606 - accuracy: 0.8649 - val_loss: 0.1202 - val_accuracy: 0.7561\n",
      "Epoch 102/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0590 - accuracy: 0.8693 - val_loss: 0.1209 - val_accuracy: 0.7687\n",
      "Epoch 103/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0588 - accuracy: 0.8697 - val_loss: 0.1123 - val_accuracy: 0.7668\n",
      "Epoch 104/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0600 - accuracy: 0.8673 - val_loss: 0.1058 - val_accuracy: 0.7826\n",
      "Epoch 105/500\n",
      "84/84 [==============================] - 22s 267ms/step - loss: 0.0580 - accuracy: 0.8711 - val_loss: 0.1153 - val_accuracy: 0.7655\n",
      "Epoch 106/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0587 - accuracy: 0.8683 - val_loss: 0.1179 - val_accuracy: 0.7697\n",
      "Epoch 107/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0580 - accuracy: 0.8705 - val_loss: 0.1133 - val_accuracy: 0.7693\n",
      "Epoch 108/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0578 - accuracy: 0.8727 - val_loss: 0.1136 - val_accuracy: 0.7647\n",
      "Epoch 109/500\n",
      "84/84 [==============================] - 22s 267ms/step - loss: 0.0580 - accuracy: 0.8681 - val_loss: 0.1190 - val_accuracy: 0.7614\n",
      "Epoch 110/500\n",
      "84/84 [==============================] - 22s 268ms/step - loss: 0.0562 - accuracy: 0.8755 - val_loss: 0.1122 - val_accuracy: 0.7632\n",
      "Epoch 111/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0583 - accuracy: 0.8704 - val_loss: 0.1048 - val_accuracy: 0.7722\n",
      "Epoch 112/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0570 - accuracy: 0.8727 - val_loss: 0.1161 - val_accuracy: 0.7637\n",
      "Epoch 113/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0583 - accuracy: 0.8697 - val_loss: 0.1332 - val_accuracy: 0.7330\n",
      "Epoch 114/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0565 - accuracy: 0.8781 - val_loss: 0.1234 - val_accuracy: 0.7668\n",
      "Epoch 115/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0567 - accuracy: 0.8731 - val_loss: 0.1247 - val_accuracy: 0.7622\n",
      "Epoch 116/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0563 - accuracy: 0.8750 - val_loss: 0.1239 - val_accuracy: 0.7643\n",
      "Epoch 117/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0569 - accuracy: 0.8770 - val_loss: 0.1089 - val_accuracy: 0.7734\n",
      "Epoch 118/500\n",
      "84/84 [==============================] - 22s 256ms/step - loss: 0.0562 - accuracy: 0.8765 - val_loss: 0.1225 - val_accuracy: 0.7582\n",
      "Epoch 119/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0563 - accuracy: 0.8750 - val_loss: 0.1200 - val_accuracy: 0.7480\n",
      "Epoch 120/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0562 - accuracy: 0.8754 - val_loss: 0.1149 - val_accuracy: 0.7649\n",
      "Epoch 121/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0555 - accuracy: 0.8756 - val_loss: 0.1186 - val_accuracy: 0.7616\n",
      "Epoch 122/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0552 - accuracy: 0.8783 - val_loss: 0.1222 - val_accuracy: 0.7478\n",
      "Epoch 123/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.0548 - accuracy: 0.8795 - val_loss: 0.1208 - val_accuracy: 0.7511\n",
      "Epoch 124/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0553 - accuracy: 0.8798 - val_loss: 0.1068 - val_accuracy: 0.7737\n",
      "Epoch 125/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0545 - accuracy: 0.8800 - val_loss: 0.1200 - val_accuracy: 0.7628\n",
      "Epoch 126/500\n",
      "84/84 [==============================] - 22s 256ms/step - loss: 0.0547 - accuracy: 0.8797 - val_loss: 0.1175 - val_accuracy: 0.7672\n",
      "Epoch 127/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0542 - accuracy: 0.8787 - val_loss: 0.1081 - val_accuracy: 0.7787\n",
      "Epoch 128/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.0534 - accuracy: 0.8822 - val_loss: 0.1361 - val_accuracy: 0.7290\n",
      "Epoch 129/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0538 - accuracy: 0.8820 - val_loss: 0.1154 - val_accuracy: 0.7664\n",
      "Epoch 130/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0534 - accuracy: 0.8828 - val_loss: 0.1163 - val_accuracy: 0.7632\n",
      "Epoch 131/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0534 - accuracy: 0.8801 - val_loss: 0.1108 - val_accuracy: 0.7634\n",
      "Epoch 132/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0539 - accuracy: 0.8796 - val_loss: 0.1057 - val_accuracy: 0.7824\n",
      "Epoch 133/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0541 - accuracy: 0.8791 - val_loss: 0.1157 - val_accuracy: 0.7686\n",
      "Epoch 134/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0535 - accuracy: 0.8809 - val_loss: 0.1071 - val_accuracy: 0.7737\n",
      "Epoch 135/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0526 - accuracy: 0.8840 - val_loss: 0.1105 - val_accuracy: 0.7810\n",
      "Epoch 136/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0535 - accuracy: 0.8838 - val_loss: 0.1178 - val_accuracy: 0.7689\n",
      "Epoch 137/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0523 - accuracy: 0.8868 - val_loss: 0.1205 - val_accuracy: 0.7755\n",
      "Epoch 138/500\n",
      "84/84 [==============================] - 21s 255ms/step - loss: 0.0532 - accuracy: 0.8838 - val_loss: 0.1112 - val_accuracy: 0.7757\n",
      "Epoch 139/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0525 - accuracy: 0.8853 - val_loss: 0.1144 - val_accuracy: 0.7787\n",
      "Epoch 140/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.0525 - accuracy: 0.8838 - val_loss: 0.1114 - val_accuracy: 0.7828\n",
      "Epoch 141/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0527 - accuracy: 0.8853 - val_loss: 0.1220 - val_accuracy: 0.7561\n",
      "Epoch 142/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0517 - accuracy: 0.8891 - val_loss: 0.1150 - val_accuracy: 0.7564\n",
      "Epoch 143/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0512 - accuracy: 0.8862 - val_loss: 0.1297 - val_accuracy: 0.7474\n",
      "Epoch 144/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0513 - accuracy: 0.8867 - val_loss: 0.1096 - val_accuracy: 0.7751\n",
      "Epoch 145/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0520 - accuracy: 0.8842 - val_loss: 0.1243 - val_accuracy: 0.7607\n",
      "Epoch 146/500\n",
      "84/84 [==============================] - 22s 267ms/step - loss: 0.0522 - accuracy: 0.8862 - val_loss: 0.1203 - val_accuracy: 0.7687\n",
      "Epoch 147/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0515 - accuracy: 0.8886 - val_loss: 0.1156 - val_accuracy: 0.7718\n",
      "Epoch 148/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0501 - accuracy: 0.8909 - val_loss: 0.1252 - val_accuracy: 0.7470\n",
      "Epoch 149/500\n",
      "84/84 [==============================] - 21s 255ms/step - loss: 0.0522 - accuracy: 0.8870 - val_loss: 0.1131 - val_accuracy: 0.7734\n",
      "Epoch 150/500\n",
      "84/84 [==============================] - 22s 267ms/step - loss: 0.0506 - accuracy: 0.8897 - val_loss: 0.1238 - val_accuracy: 0.7586\n",
      "Epoch 151/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0508 - accuracy: 0.8877 - val_loss: 0.1253 - val_accuracy: 0.7655\n",
      "Epoch 152/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0497 - accuracy: 0.8925 - val_loss: 0.1354 - val_accuracy: 0.7528\n",
      "Epoch 153/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.0490 - accuracy: 0.8922 - val_loss: 0.1159 - val_accuracy: 0.7749\n",
      "Epoch 154/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0502 - accuracy: 0.8909 - val_loss: 0.1227 - val_accuracy: 0.7451\n",
      "Epoch 155/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0507 - accuracy: 0.8898 - val_loss: 0.1239 - val_accuracy: 0.7593\n",
      "Epoch 156/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0499 - accuracy: 0.8929 - val_loss: 0.1278 - val_accuracy: 0.7591\n",
      "Epoch 157/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0517 - accuracy: 0.8877 - val_loss: 0.1280 - val_accuracy: 0.7661\n",
      "Epoch 158/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0488 - accuracy: 0.8929 - val_loss: 0.1152 - val_accuracy: 0.7710\n",
      "Epoch 159/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0490 - accuracy: 0.8928 - val_loss: 0.1182 - val_accuracy: 0.7705\n",
      "Epoch 160/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0486 - accuracy: 0.8925 - val_loss: 0.1321 - val_accuracy: 0.7651\n",
      "Epoch 161/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0496 - accuracy: 0.8936 - val_loss: 0.1141 - val_accuracy: 0.7722\n",
      "Epoch 162/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0488 - accuracy: 0.8957 - val_loss: 0.1231 - val_accuracy: 0.7601\n",
      "Epoch 163/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0492 - accuracy: 0.8915 - val_loss: 0.1170 - val_accuracy: 0.7780\n",
      "Epoch 164/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0485 - accuracy: 0.8968 - val_loss: 0.1326 - val_accuracy: 0.7464\n",
      "Epoch 165/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0498 - accuracy: 0.8939 - val_loss: 0.1195 - val_accuracy: 0.7735\n",
      "Epoch 166/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0483 - accuracy: 0.8956 - val_loss: 0.1276 - val_accuracy: 0.7518\n",
      "Epoch 167/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0491 - accuracy: 0.8925 - val_loss: 0.1293 - val_accuracy: 0.7486\n",
      "Epoch 168/500\n",
      "84/84 [==============================] - 22s 256ms/step - loss: 0.0488 - accuracy: 0.8942 - val_loss: 0.1102 - val_accuracy: 0.7787\n",
      "Epoch 169/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0490 - accuracy: 0.8937 - val_loss: 0.1200 - val_accuracy: 0.7707\n",
      "Epoch 170/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0489 - accuracy: 0.8949 - val_loss: 0.1175 - val_accuracy: 0.7760\n",
      "Epoch 171/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0476 - accuracy: 0.8972 - val_loss: 0.1236 - val_accuracy: 0.7810\n",
      "Epoch 172/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0466 - accuracy: 0.9000 - val_loss: 0.1190 - val_accuracy: 0.7766\n",
      "Epoch 173/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0479 - accuracy: 0.8963 - val_loss: 0.1344 - val_accuracy: 0.7497\n",
      "Epoch 174/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0482 - accuracy: 0.8979 - val_loss: 0.1187 - val_accuracy: 0.7737\n",
      "Epoch 175/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0473 - accuracy: 0.8986 - val_loss: 0.1198 - val_accuracy: 0.7614\n",
      "Epoch 176/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0478 - accuracy: 0.8973 - val_loss: 0.1124 - val_accuracy: 0.7809\n",
      "Epoch 177/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0474 - accuracy: 0.8988 - val_loss: 0.1175 - val_accuracy: 0.7755\n",
      "Epoch 178/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0482 - accuracy: 0.8966 - val_loss: 0.1367 - val_accuracy: 0.7587\n",
      "Epoch 179/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0483 - accuracy: 0.8960 - val_loss: 0.1197 - val_accuracy: 0.7716\n",
      "Epoch 180/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0473 - accuracy: 0.8985 - val_loss: 0.1143 - val_accuracy: 0.7784\n",
      "Epoch 181/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0466 - accuracy: 0.9010 - val_loss: 0.1254 - val_accuracy: 0.7689\n",
      "Epoch 182/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0467 - accuracy: 0.9006 - val_loss: 0.1261 - val_accuracy: 0.7686\n",
      "Epoch 183/500\n",
      "84/84 [==============================] - 22s 267ms/step - loss: 0.0468 - accuracy: 0.8982 - val_loss: 0.1360 - val_accuracy: 0.7372\n",
      "Epoch 184/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0466 - accuracy: 0.9019 - val_loss: 0.1379 - val_accuracy: 0.7424\n",
      "Epoch 185/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0464 - accuracy: 0.9002 - val_loss: 0.1154 - val_accuracy: 0.7686\n",
      "Epoch 186/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0461 - accuracy: 0.8999 - val_loss: 0.1274 - val_accuracy: 0.7553\n",
      "Epoch 187/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0466 - accuracy: 0.9016 - val_loss: 0.1377 - val_accuracy: 0.7614\n",
      "Epoch 188/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0464 - accuracy: 0.9002 - val_loss: 0.1255 - val_accuracy: 0.7712\n",
      "Epoch 189/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0458 - accuracy: 0.8995 - val_loss: 0.1172 - val_accuracy: 0.7709\n",
      "Epoch 190/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0455 - accuracy: 0.9026 - val_loss: 0.1174 - val_accuracy: 0.7718\n",
      "Epoch 191/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0459 - accuracy: 0.9005 - val_loss: 0.1276 - val_accuracy: 0.7472\n",
      "Epoch 192/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0470 - accuracy: 0.8989 - val_loss: 0.1378 - val_accuracy: 0.7553\n",
      "Epoch 193/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0459 - accuracy: 0.8987 - val_loss: 0.1329 - val_accuracy: 0.7512\n",
      "Epoch 194/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0452 - accuracy: 0.9041 - val_loss: 0.1310 - val_accuracy: 0.7599\n",
      "Epoch 195/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0449 - accuracy: 0.9008 - val_loss: 0.1205 - val_accuracy: 0.7759\n",
      "Epoch 196/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0443 - accuracy: 0.9070 - val_loss: 0.1136 - val_accuracy: 0.7745\n",
      "Epoch 197/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0464 - accuracy: 0.9001 - val_loss: 0.1283 - val_accuracy: 0.7572\n",
      "Epoch 198/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0449 - accuracy: 0.9041 - val_loss: 0.1197 - val_accuracy: 0.7734\n",
      "Epoch 199/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0458 - accuracy: 0.9032 - val_loss: 0.1229 - val_accuracy: 0.7710\n",
      "Epoch 200/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0446 - accuracy: 0.9047 - val_loss: 0.1200 - val_accuracy: 0.7730\n",
      "Epoch 201/500\n",
      "84/84 [==============================] - 21s 256ms/step - loss: 0.0438 - accuracy: 0.9057 - val_loss: 0.1385 - val_accuracy: 0.7666\n",
      "Epoch 202/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0458 - accuracy: 0.9037 - val_loss: 0.1313 - val_accuracy: 0.7572\n",
      "Epoch 203/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0448 - accuracy: 0.9028 - val_loss: 0.1424 - val_accuracy: 0.7493\n",
      "Epoch 204/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0440 - accuracy: 0.9070 - val_loss: 0.1318 - val_accuracy: 0.7714\n",
      "Epoch 205/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0435 - accuracy: 0.9080 - val_loss: 0.1285 - val_accuracy: 0.7661\n",
      "Epoch 206/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0443 - accuracy: 0.9063 - val_loss: 0.1303 - val_accuracy: 0.7553\n",
      "Epoch 207/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0435 - accuracy: 0.9091 - val_loss: 0.1290 - val_accuracy: 0.7716\n",
      "Epoch 208/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0444 - accuracy: 0.9063 - val_loss: 0.1252 - val_accuracy: 0.7728\n",
      "Epoch 209/500\n",
      "84/84 [==============================] - 22s 267ms/step - loss: 0.0427 - accuracy: 0.9117 - val_loss: 0.1262 - val_accuracy: 0.7730\n",
      "Epoch 210/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0428 - accuracy: 0.9115 - val_loss: 0.1422 - val_accuracy: 0.7488\n",
      "Epoch 211/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0446 - accuracy: 0.9071 - val_loss: 0.1258 - val_accuracy: 0.7701\n",
      "Epoch 212/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0439 - accuracy: 0.9072 - val_loss: 0.1198 - val_accuracy: 0.7724\n",
      "Epoch 213/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0437 - accuracy: 0.9072 - val_loss: 0.1296 - val_accuracy: 0.7641\n",
      "Epoch 214/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0427 - accuracy: 0.9105 - val_loss: 0.1225 - val_accuracy: 0.7710\n",
      "Epoch 215/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.0438 - accuracy: 0.9086 - val_loss: 0.1246 - val_accuracy: 0.7678\n",
      "Epoch 216/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0429 - accuracy: 0.9086 - val_loss: 0.1346 - val_accuracy: 0.7630\n",
      "Epoch 217/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0439 - accuracy: 0.9092 - val_loss: 0.1262 - val_accuracy: 0.7562\n",
      "Epoch 218/500\n",
      "84/84 [==============================] - 21s 256ms/step - loss: 0.0436 - accuracy: 0.9101 - val_loss: 0.1282 - val_accuracy: 0.7622\n",
      "Epoch 219/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0434 - accuracy: 0.9109 - val_loss: 0.1210 - val_accuracy: 0.7845\n",
      "Epoch 220/500\n",
      "84/84 [==============================] - 21s 256ms/step - loss: 0.0440 - accuracy: 0.9083 - val_loss: 0.1212 - val_accuracy: 0.7759\n",
      "Epoch 221/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0432 - accuracy: 0.9098 - val_loss: 0.1190 - val_accuracy: 0.7812\n",
      "Epoch 222/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0423 - accuracy: 0.9102 - val_loss: 0.1217 - val_accuracy: 0.7759\n",
      "Epoch 223/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0429 - accuracy: 0.9084 - val_loss: 0.1245 - val_accuracy: 0.7762\n",
      "Epoch 224/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0428 - accuracy: 0.9098 - val_loss: 0.1276 - val_accuracy: 0.7776\n",
      "Epoch 225/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0430 - accuracy: 0.9090 - val_loss: 0.1286 - val_accuracy: 0.7637\n",
      "Epoch 226/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0420 - accuracy: 0.9107 - val_loss: 0.1367 - val_accuracy: 0.7449\n",
      "Epoch 227/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0423 - accuracy: 0.9116 - val_loss: 0.1209 - val_accuracy: 0.7699\n",
      "Epoch 228/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0410 - accuracy: 0.9144 - val_loss: 0.1282 - val_accuracy: 0.7641\n",
      "Epoch 229/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0432 - accuracy: 0.9096 - val_loss: 0.1343 - val_accuracy: 0.7643\n",
      "Epoch 230/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0412 - accuracy: 0.9159 - val_loss: 0.1262 - val_accuracy: 0.7770\n",
      "Epoch 231/500\n",
      "84/84 [==============================] - 23s 268ms/step - loss: 0.0418 - accuracy: 0.9124 - val_loss: 0.1366 - val_accuracy: 0.7636\n",
      "Epoch 232/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0433 - accuracy: 0.9081 - val_loss: 0.1340 - val_accuracy: 0.7572\n",
      "Epoch 233/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0412 - accuracy: 0.9112 - val_loss: 0.1294 - val_accuracy: 0.7659\n",
      "Epoch 234/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0427 - accuracy: 0.9104 - val_loss: 0.1236 - val_accuracy: 0.7760\n",
      "Epoch 235/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0418 - accuracy: 0.9131 - val_loss: 0.1307 - val_accuracy: 0.7618\n",
      "Epoch 236/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0411 - accuracy: 0.9143 - val_loss: 0.1274 - val_accuracy: 0.7686\n",
      "Epoch 237/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0409 - accuracy: 0.9157 - val_loss: 0.1220 - val_accuracy: 0.7734\n",
      "Epoch 238/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0421 - accuracy: 0.9103 - val_loss: 0.1346 - val_accuracy: 0.7566\n",
      "Epoch 239/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0426 - accuracy: 0.9092 - val_loss: 0.1472 - val_accuracy: 0.7509\n",
      "Epoch 240/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0412 - accuracy: 0.9129 - val_loss: 0.1326 - val_accuracy: 0.7657\n",
      "Epoch 241/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0413 - accuracy: 0.9143 - val_loss: 0.1275 - val_accuracy: 0.7659\n",
      "Epoch 242/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0401 - accuracy: 0.9159 - val_loss: 0.1322 - val_accuracy: 0.7712\n",
      "Epoch 243/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0405 - accuracy: 0.9147 - val_loss: 0.1334 - val_accuracy: 0.7609\n",
      "Epoch 244/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0433 - accuracy: 0.9080 - val_loss: 0.1272 - val_accuracy: 0.7670\n",
      "Epoch 245/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0395 - accuracy: 0.9178 - val_loss: 0.1250 - val_accuracy: 0.7695\n",
      "Epoch 246/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0405 - accuracy: 0.9137 - val_loss: 0.1324 - val_accuracy: 0.7505\n",
      "Epoch 247/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0399 - accuracy: 0.9174 - val_loss: 0.1364 - val_accuracy: 0.7678\n",
      "Epoch 248/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0411 - accuracy: 0.9155 - val_loss: 0.1241 - val_accuracy: 0.7661\n",
      "Epoch 249/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0400 - accuracy: 0.9176 - val_loss: 0.1333 - val_accuracy: 0.7722\n",
      "Epoch 250/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0396 - accuracy: 0.9169 - val_loss: 0.1283 - val_accuracy: 0.7689\n",
      "Epoch 251/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0405 - accuracy: 0.9161 - val_loss: 0.1350 - val_accuracy: 0.7662\n",
      "Epoch 252/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0399 - accuracy: 0.9164 - val_loss: 0.1296 - val_accuracy: 0.7603\n",
      "Epoch 253/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0402 - accuracy: 0.9159 - val_loss: 0.1340 - val_accuracy: 0.7657\n",
      "Epoch 254/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0395 - accuracy: 0.9182 - val_loss: 0.1287 - val_accuracy: 0.7680\n",
      "Epoch 255/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0398 - accuracy: 0.9170 - val_loss: 0.1243 - val_accuracy: 0.7670\n",
      "Epoch 256/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0393 - accuracy: 0.9184 - val_loss: 0.1226 - val_accuracy: 0.7797\n",
      "Epoch 257/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0390 - accuracy: 0.9194 - val_loss: 0.1390 - val_accuracy: 0.7572\n",
      "Epoch 258/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0393 - accuracy: 0.9189 - val_loss: 0.1315 - val_accuracy: 0.7657\n",
      "Epoch 259/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0406 - accuracy: 0.9138 - val_loss: 0.1442 - val_accuracy: 0.7624\n",
      "Epoch 260/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0396 - accuracy: 0.9176 - val_loss: 0.1226 - val_accuracy: 0.7741\n",
      "Epoch 261/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0405 - accuracy: 0.9162 - val_loss: 0.1212 - val_accuracy: 0.7759\n",
      "Epoch 262/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0391 - accuracy: 0.9192 - val_loss: 0.1297 - val_accuracy: 0.7737\n",
      "Epoch 263/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0393 - accuracy: 0.9196 - val_loss: 0.1272 - val_accuracy: 0.7662\n",
      "Epoch 264/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0397 - accuracy: 0.9177 - val_loss: 0.1362 - val_accuracy: 0.7599\n",
      "Epoch 265/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0408 - accuracy: 0.9141 - val_loss: 0.1356 - val_accuracy: 0.7576\n",
      "Epoch 266/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0388 - accuracy: 0.9204 - val_loss: 0.1342 - val_accuracy: 0.7555\n",
      "Epoch 267/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0387 - accuracy: 0.9204 - val_loss: 0.1412 - val_accuracy: 0.7624\n",
      "Epoch 268/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0389 - accuracy: 0.9187 - val_loss: 0.1248 - val_accuracy: 0.7768\n",
      "Epoch 269/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0395 - accuracy: 0.9192 - val_loss: 0.1337 - val_accuracy: 0.7518\n",
      "Epoch 270/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0403 - accuracy: 0.9144 - val_loss: 0.1236 - val_accuracy: 0.7762\n",
      "Epoch 271/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0391 - accuracy: 0.9189 - val_loss: 0.1249 - val_accuracy: 0.7764\n",
      "Epoch 272/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0386 - accuracy: 0.9209 - val_loss: 0.1285 - val_accuracy: 0.7730\n",
      "Epoch 273/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0387 - accuracy: 0.9209 - val_loss: 0.1270 - val_accuracy: 0.7643\n",
      "Epoch 274/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0386 - accuracy: 0.9203 - val_loss: 0.1283 - val_accuracy: 0.7695\n",
      "Epoch 275/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0378 - accuracy: 0.9220 - val_loss: 0.1286 - val_accuracy: 0.7693\n",
      "Epoch 276/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0393 - accuracy: 0.9175 - val_loss: 0.1277 - val_accuracy: 0.7668\n",
      "Epoch 277/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0379 - accuracy: 0.9220 - val_loss: 0.1442 - val_accuracy: 0.7618\n",
      "Epoch 278/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0381 - accuracy: 0.9210 - val_loss: 0.1338 - val_accuracy: 0.7589\n",
      "Epoch 279/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0387 - accuracy: 0.9203 - val_loss: 0.1397 - val_accuracy: 0.7616\n",
      "Epoch 280/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0372 - accuracy: 0.9257 - val_loss: 0.1395 - val_accuracy: 0.7674\n",
      "Epoch 281/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0387 - accuracy: 0.9200 - val_loss: 0.1305 - val_accuracy: 0.7655\n",
      "Epoch 282/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0381 - accuracy: 0.9213 - val_loss: 0.1358 - val_accuracy: 0.7693\n",
      "Epoch 283/500\n",
      "84/84 [==============================] - 21s 254ms/step - loss: 0.0376 - accuracy: 0.9219 - val_loss: 0.1464 - val_accuracy: 0.7476\n",
      "Epoch 284/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0374 - accuracy: 0.9232 - val_loss: 0.1382 - val_accuracy: 0.7726\n",
      "Epoch 285/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0375 - accuracy: 0.9238 - val_loss: 0.1385 - val_accuracy: 0.7549\n",
      "Epoch 286/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0370 - accuracy: 0.9246 - val_loss: 0.1458 - val_accuracy: 0.7491\n",
      "Epoch 287/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0383 - accuracy: 0.9232 - val_loss: 0.1292 - val_accuracy: 0.7714\n",
      "Epoch 288/500\n",
      "84/84 [==============================] - 21s 255ms/step - loss: 0.0379 - accuracy: 0.9246 - val_loss: 0.1422 - val_accuracy: 0.7505\n",
      "Epoch 289/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0375 - accuracy: 0.9236 - val_loss: 0.1327 - val_accuracy: 0.7745\n",
      "Epoch 290/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0382 - accuracy: 0.9198 - val_loss: 0.1331 - val_accuracy: 0.7691\n",
      "Epoch 291/500\n",
      "84/84 [==============================] - 21s 255ms/step - loss: 0.0357 - accuracy: 0.9277 - val_loss: 0.1312 - val_accuracy: 0.7762\n",
      "Epoch 292/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0370 - accuracy: 0.9251 - val_loss: 0.1569 - val_accuracy: 0.7576\n",
      "Epoch 293/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0375 - accuracy: 0.9220 - val_loss: 0.1406 - val_accuracy: 0.7643\n",
      "Epoch 294/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0364 - accuracy: 0.9259 - val_loss: 0.1331 - val_accuracy: 0.7728\n",
      "Epoch 295/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0372 - accuracy: 0.9253 - val_loss: 0.1386 - val_accuracy: 0.7605\n",
      "Epoch 296/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0374 - accuracy: 0.9214 - val_loss: 0.1405 - val_accuracy: 0.7566\n",
      "Epoch 297/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0361 - accuracy: 0.9261 - val_loss: 0.1354 - val_accuracy: 0.7632\n",
      "Epoch 298/500\n",
      "84/84 [==============================] - 23s 269ms/step - loss: 0.0373 - accuracy: 0.9216 - val_loss: 0.1388 - val_accuracy: 0.7701\n",
      "Epoch 299/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0378 - accuracy: 0.9243 - val_loss: 0.1317 - val_accuracy: 0.7670\n",
      "Epoch 300/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0373 - accuracy: 0.9245 - val_loss: 0.1522 - val_accuracy: 0.7534\n",
      "Epoch 301/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0364 - accuracy: 0.9257 - val_loss: 0.1363 - val_accuracy: 0.7649\n",
      "Epoch 302/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0362 - accuracy: 0.9271 - val_loss: 0.1354 - val_accuracy: 0.7647\n",
      "Epoch 303/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0362 - accuracy: 0.9264 - val_loss: 0.1285 - val_accuracy: 0.7678\n",
      "Epoch 304/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0369 - accuracy: 0.9219 - val_loss: 0.1497 - val_accuracy: 0.7720\n",
      "Epoch 305/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0378 - accuracy: 0.9228 - val_loss: 0.1425 - val_accuracy: 0.7507\n",
      "Epoch 306/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0365 - accuracy: 0.9270 - val_loss: 0.1601 - val_accuracy: 0.7326\n",
      "Epoch 307/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0359 - accuracy: 0.9249 - val_loss: 0.1431 - val_accuracy: 0.7639\n",
      "Epoch 308/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0365 - accuracy: 0.9264 - val_loss: 0.1289 - val_accuracy: 0.7816\n",
      "Epoch 309/500\n",
      "84/84 [==============================] - 22s 267ms/step - loss: 0.0359 - accuracy: 0.9263 - val_loss: 0.1467 - val_accuracy: 0.7649\n",
      "Epoch 310/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0363 - accuracy: 0.9234 - val_loss: 0.1364 - val_accuracy: 0.7766\n",
      "Epoch 311/500\n",
      "84/84 [==============================] - 21s 256ms/step - loss: 0.0359 - accuracy: 0.9283 - val_loss: 0.1330 - val_accuracy: 0.7686\n",
      "Epoch 312/500\n",
      "84/84 [==============================] - 21s 255ms/step - loss: 0.0358 - accuracy: 0.9283 - val_loss: 0.1328 - val_accuracy: 0.7749\n",
      "Epoch 313/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0366 - accuracy: 0.9255 - val_loss: 0.1370 - val_accuracy: 0.7737\n",
      "Epoch 314/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0370 - accuracy: 0.9244 - val_loss: 0.1293 - val_accuracy: 0.7762\n",
      "Epoch 315/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0370 - accuracy: 0.9263 - val_loss: 0.1389 - val_accuracy: 0.7639\n",
      "Epoch 316/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0363 - accuracy: 0.9267 - val_loss: 0.1388 - val_accuracy: 0.7536\n",
      "Epoch 317/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0362 - accuracy: 0.9257 - val_loss: 0.1430 - val_accuracy: 0.7595\n",
      "Epoch 318/500\n",
      "84/84 [==============================] - 22s 256ms/step - loss: 0.0358 - accuracy: 0.9287 - val_loss: 0.1348 - val_accuracy: 0.7684\n",
      "Epoch 319/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0372 - accuracy: 0.9263 - val_loss: 0.1391 - val_accuracy: 0.7689\n",
      "Epoch 320/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0360 - accuracy: 0.9266 - val_loss: 0.1313 - val_accuracy: 0.7747\n",
      "Epoch 321/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0345 - accuracy: 0.9299 - val_loss: 0.1435 - val_accuracy: 0.7693\n",
      "Epoch 322/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0355 - accuracy: 0.9265 - val_loss: 0.1356 - val_accuracy: 0.7766\n",
      "Epoch 323/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0357 - accuracy: 0.9295 - val_loss: 0.1352 - val_accuracy: 0.7662\n",
      "Epoch 324/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0375 - accuracy: 0.9243 - val_loss: 0.1527 - val_accuracy: 0.7536\n",
      "Epoch 325/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0361 - accuracy: 0.9270 - val_loss: 0.1383 - val_accuracy: 0.7672\n",
      "Epoch 326/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0368 - accuracy: 0.9262 - val_loss: 0.1323 - val_accuracy: 0.7647\n",
      "Epoch 327/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0345 - accuracy: 0.9291 - val_loss: 0.1351 - val_accuracy: 0.7705\n",
      "Epoch 328/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0347 - accuracy: 0.9295 - val_loss: 0.1352 - val_accuracy: 0.7647\n",
      "Epoch 329/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0355 - accuracy: 0.9275 - val_loss: 0.1498 - val_accuracy: 0.7649\n",
      "Epoch 330/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0352 - accuracy: 0.9291 - val_loss: 0.1337 - val_accuracy: 0.7743\n",
      "Epoch 331/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0348 - accuracy: 0.9316 - val_loss: 0.1437 - val_accuracy: 0.7670\n",
      "Epoch 332/500\n",
      "84/84 [==============================] - 22s 268ms/step - loss: 0.0351 - accuracy: 0.9286 - val_loss: 0.1441 - val_accuracy: 0.7662\n",
      "Epoch 333/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0349 - accuracy: 0.9284 - val_loss: 0.1379 - val_accuracy: 0.7647\n",
      "Epoch 334/500\n",
      "84/84 [==============================] - 22s 256ms/step - loss: 0.0351 - accuracy: 0.9284 - val_loss: 0.1325 - val_accuracy: 0.7687\n",
      "Epoch 335/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0352 - accuracy: 0.9282 - val_loss: 0.1394 - val_accuracy: 0.7730\n",
      "Epoch 336/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0346 - accuracy: 0.9303 - val_loss: 0.1440 - val_accuracy: 0.7607\n",
      "Epoch 337/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0342 - accuracy: 0.9324 - val_loss: 0.1363 - val_accuracy: 0.7801\n",
      "Epoch 338/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0336 - accuracy: 0.9332 - val_loss: 0.1403 - val_accuracy: 0.7686\n",
      "Epoch 339/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0339 - accuracy: 0.9315 - val_loss: 0.1313 - val_accuracy: 0.7747\n",
      "Epoch 340/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0350 - accuracy: 0.9303 - val_loss: 0.1332 - val_accuracy: 0.7653\n",
      "Epoch 341/500\n",
      "84/84 [==============================] - 21s 256ms/step - loss: 0.0342 - accuracy: 0.9307 - val_loss: 0.1352 - val_accuracy: 0.7661\n",
      "Epoch 342/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0345 - accuracy: 0.9307 - val_loss: 0.1405 - val_accuracy: 0.7712\n",
      "Epoch 343/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0358 - accuracy: 0.9294 - val_loss: 0.1392 - val_accuracy: 0.7686\n",
      "Epoch 344/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0342 - accuracy: 0.9306 - val_loss: 0.1604 - val_accuracy: 0.7439\n",
      "Epoch 345/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0331 - accuracy: 0.9342 - val_loss: 0.1468 - val_accuracy: 0.7678\n",
      "Epoch 346/500\n",
      "84/84 [==============================] - 22s 267ms/step - loss: 0.0355 - accuracy: 0.9293 - val_loss: 0.1493 - val_accuracy: 0.7511\n",
      "Epoch 347/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0355 - accuracy: 0.9301 - val_loss: 0.1542 - val_accuracy: 0.7661\n",
      "Epoch 348/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0350 - accuracy: 0.9292 - val_loss: 0.1400 - val_accuracy: 0.7791\n",
      "Epoch 349/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0334 - accuracy: 0.9337 - val_loss: 0.1349 - val_accuracy: 0.7693\n",
      "Epoch 350/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.0333 - accuracy: 0.9327 - val_loss: 0.1344 - val_accuracy: 0.7776\n",
      "Epoch 351/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0326 - accuracy: 0.9354 - val_loss: 0.1372 - val_accuracy: 0.7718\n",
      "Epoch 352/500\n",
      "84/84 [==============================] - 22s 256ms/step - loss: 0.0345 - accuracy: 0.9319 - val_loss: 0.1417 - val_accuracy: 0.7732\n",
      "Epoch 353/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0342 - accuracy: 0.9331 - val_loss: 0.1445 - val_accuracy: 0.7651\n",
      "Epoch 354/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0342 - accuracy: 0.9330 - val_loss: 0.1360 - val_accuracy: 0.7684\n",
      "Epoch 355/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0338 - accuracy: 0.9321 - val_loss: 0.1424 - val_accuracy: 0.7680\n",
      "Epoch 356/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0339 - accuracy: 0.9310 - val_loss: 0.1474 - val_accuracy: 0.7518\n",
      "Epoch 357/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0341 - accuracy: 0.9310 - val_loss: 0.1361 - val_accuracy: 0.7759\n",
      "Epoch 358/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.0335 - accuracy: 0.9332 - val_loss: 0.1347 - val_accuracy: 0.7780\n",
      "Epoch 359/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0329 - accuracy: 0.9342 - val_loss: 0.1454 - val_accuracy: 0.7682\n",
      "Epoch 360/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0360 - accuracy: 0.9276 - val_loss: 0.1481 - val_accuracy: 0.7649\n",
      "Epoch 361/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0348 - accuracy: 0.9298 - val_loss: 0.1385 - val_accuracy: 0.7730\n",
      "Epoch 362/500\n",
      "84/84 [==============================] - 21s 254ms/step - loss: 0.0348 - accuracy: 0.9290 - val_loss: 0.1375 - val_accuracy: 0.7693\n",
      "Epoch 363/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0322 - accuracy: 0.9366 - val_loss: 0.1400 - val_accuracy: 0.7605\n",
      "Epoch 364/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0332 - accuracy: 0.9334 - val_loss: 0.1482 - val_accuracy: 0.7643\n",
      "Epoch 365/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0337 - accuracy: 0.9337 - val_loss: 0.1496 - val_accuracy: 0.7555\n",
      "Epoch 366/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0330 - accuracy: 0.9358 - val_loss: 0.1470 - val_accuracy: 0.7553\n",
      "Epoch 367/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0339 - accuracy: 0.9318 - val_loss: 0.1417 - val_accuracy: 0.7612\n",
      "Epoch 368/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0332 - accuracy: 0.9347 - val_loss: 0.1561 - val_accuracy: 0.7537\n",
      "Epoch 369/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0344 - accuracy: 0.9300 - val_loss: 0.1408 - val_accuracy: 0.7722\n",
      "Epoch 370/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0334 - accuracy: 0.9324 - val_loss: 0.1339 - val_accuracy: 0.7739\n",
      "Epoch 371/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0331 - accuracy: 0.9323 - val_loss: 0.1469 - val_accuracy: 0.7568\n",
      "Epoch 372/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0338 - accuracy: 0.9317 - val_loss: 0.1426 - val_accuracy: 0.7743\n",
      "Epoch 373/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0326 - accuracy: 0.9339 - val_loss: 0.1517 - val_accuracy: 0.7662\n",
      "Epoch 374/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0342 - accuracy: 0.9348 - val_loss: 0.1505 - val_accuracy: 0.7739\n",
      "Epoch 375/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0329 - accuracy: 0.9340 - val_loss: 0.1409 - val_accuracy: 0.7749\n",
      "Epoch 376/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0330 - accuracy: 0.9339 - val_loss: 0.1459 - val_accuracy: 0.7780\n",
      "Epoch 377/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0341 - accuracy: 0.9323 - val_loss: 0.1494 - val_accuracy: 0.7574\n",
      "Epoch 378/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0328 - accuracy: 0.9360 - val_loss: 0.1482 - val_accuracy: 0.7541\n",
      "Epoch 379/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0321 - accuracy: 0.9366 - val_loss: 0.1449 - val_accuracy: 0.7603\n",
      "Epoch 380/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0318 - accuracy: 0.9391 - val_loss: 0.1576 - val_accuracy: 0.7522\n",
      "Epoch 381/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0333 - accuracy: 0.9334 - val_loss: 0.1559 - val_accuracy: 0.7572\n",
      "Epoch 382/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0346 - accuracy: 0.9304 - val_loss: 0.1337 - val_accuracy: 0.7732\n",
      "Epoch 383/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0323 - accuracy: 0.9364 - val_loss: 0.1363 - val_accuracy: 0.7661\n",
      "Epoch 384/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0328 - accuracy: 0.9364 - val_loss: 0.1493 - val_accuracy: 0.7537\n",
      "Epoch 385/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0334 - accuracy: 0.9333 - val_loss: 0.1452 - val_accuracy: 0.7620\n",
      "Epoch 386/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0324 - accuracy: 0.9348 - val_loss: 0.1459 - val_accuracy: 0.7618\n",
      "Epoch 387/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0326 - accuracy: 0.9360 - val_loss: 0.1361 - val_accuracy: 0.7728\n",
      "Epoch 388/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0330 - accuracy: 0.9351 - val_loss: 0.1519 - val_accuracy: 0.7551\n",
      "Epoch 389/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0318 - accuracy: 0.9365 - val_loss: 0.1478 - val_accuracy: 0.7641\n",
      "Epoch 390/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0335 - accuracy: 0.9329 - val_loss: 0.1414 - val_accuracy: 0.7709\n",
      "Epoch 391/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0327 - accuracy: 0.9349 - val_loss: 0.1476 - val_accuracy: 0.7584\n",
      "Epoch 392/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0327 - accuracy: 0.9352 - val_loss: 0.1406 - val_accuracy: 0.7799\n",
      "Epoch 393/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0319 - accuracy: 0.9384 - val_loss: 0.1387 - val_accuracy: 0.7712\n",
      "Epoch 394/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0316 - accuracy: 0.9379 - val_loss: 0.1326 - val_accuracy: 0.7789\n",
      "Epoch 395/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0331 - accuracy: 0.9352 - val_loss: 0.1454 - val_accuracy: 0.7649\n",
      "Epoch 396/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0336 - accuracy: 0.9331 - val_loss: 0.1476 - val_accuracy: 0.7537\n",
      "Epoch 397/500\n",
      "84/84 [==============================] - 21s 255ms/step - loss: 0.0316 - accuracy: 0.9381 - val_loss: 0.1412 - val_accuracy: 0.7747\n",
      "Epoch 398/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0331 - accuracy: 0.9339 - val_loss: 0.1447 - val_accuracy: 0.7561\n",
      "Epoch 399/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0342 - accuracy: 0.9311 - val_loss: 0.1702 - val_accuracy: 0.7426\n",
      "Epoch 400/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0321 - accuracy: 0.9359 - val_loss: 0.1454 - val_accuracy: 0.7691\n",
      "Epoch 401/500\n",
      "84/84 [==============================] - 22s 267ms/step - loss: 0.0321 - accuracy: 0.9374 - val_loss: 0.1376 - val_accuracy: 0.7684\n",
      "Epoch 402/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0319 - accuracy: 0.9365 - val_loss: 0.1385 - val_accuracy: 0.7749\n",
      "Epoch 403/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0317 - accuracy: 0.9389 - val_loss: 0.1366 - val_accuracy: 0.7809\n",
      "Epoch 404/500\n",
      "84/84 [==============================] - 22s 256ms/step - loss: 0.0316 - accuracy: 0.9369 - val_loss: 0.1429 - val_accuracy: 0.7661\n",
      "Epoch 405/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0327 - accuracy: 0.9369 - val_loss: 0.1513 - val_accuracy: 0.7612\n",
      "Epoch 406/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0310 - accuracy: 0.9390 - val_loss: 0.1501 - val_accuracy: 0.7564\n",
      "Epoch 407/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0320 - accuracy: 0.9388 - val_loss: 0.1461 - val_accuracy: 0.7616\n",
      "Epoch 408/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0306 - accuracy: 0.9399 - val_loss: 0.1455 - val_accuracy: 0.7682\n",
      "Epoch 409/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0316 - accuracy: 0.9384 - val_loss: 0.1430 - val_accuracy: 0.7666\n",
      "Epoch 410/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0314 - accuracy: 0.9381 - val_loss: 0.1555 - val_accuracy: 0.7612\n",
      "Epoch 411/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0310 - accuracy: 0.9398 - val_loss: 0.1473 - val_accuracy: 0.7670\n",
      "Epoch 412/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0319 - accuracy: 0.9375 - val_loss: 0.1602 - val_accuracy: 0.7553\n",
      "Epoch 413/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0312 - accuracy: 0.9385 - val_loss: 0.1356 - val_accuracy: 0.7703\n",
      "Epoch 414/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0319 - accuracy: 0.9366 - val_loss: 0.1463 - val_accuracy: 0.7618\n",
      "Epoch 415/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0307 - accuracy: 0.9400 - val_loss: 0.1538 - val_accuracy: 0.7536\n",
      "Epoch 416/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0319 - accuracy: 0.9366 - val_loss: 0.1354 - val_accuracy: 0.7668\n",
      "Epoch 417/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0320 - accuracy: 0.9350 - val_loss: 0.1437 - val_accuracy: 0.7618\n",
      "Epoch 418/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0319 - accuracy: 0.9387 - val_loss: 0.1528 - val_accuracy: 0.7459\n",
      "Epoch 419/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0305 - accuracy: 0.9428 - val_loss: 0.1415 - val_accuracy: 0.7724\n",
      "Epoch 420/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0294 - accuracy: 0.9443 - val_loss: 0.1433 - val_accuracy: 0.7672\n",
      "Epoch 421/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0318 - accuracy: 0.9375 - val_loss: 0.1446 - val_accuracy: 0.7689\n",
      "Epoch 422/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0291 - accuracy: 0.9419 - val_loss: 0.1430 - val_accuracy: 0.7622\n",
      "Epoch 423/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0316 - accuracy: 0.9386 - val_loss: 0.1419 - val_accuracy: 0.7772\n",
      "Epoch 424/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0310 - accuracy: 0.9394 - val_loss: 0.1479 - val_accuracy: 0.7607\n",
      "Epoch 425/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0297 - accuracy: 0.9408 - val_loss: 0.1488 - val_accuracy: 0.7564\n",
      "Epoch 426/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0308 - accuracy: 0.9397 - val_loss: 0.1488 - val_accuracy: 0.7659\n",
      "Epoch 427/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0303 - accuracy: 0.9422 - val_loss: 0.1422 - val_accuracy: 0.7695\n",
      "Epoch 428/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0304 - accuracy: 0.9400 - val_loss: 0.1536 - val_accuracy: 0.7701\n",
      "Epoch 429/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.0317 - accuracy: 0.9377 - val_loss: 0.1374 - val_accuracy: 0.7730\n",
      "Epoch 430/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0322 - accuracy: 0.9358 - val_loss: 0.1399 - val_accuracy: 0.7657\n",
      "Epoch 431/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0300 - accuracy: 0.9417 - val_loss: 0.1497 - val_accuracy: 0.7751\n",
      "Epoch 432/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.0298 - accuracy: 0.9403 - val_loss: 0.1433 - val_accuracy: 0.7760\n",
      "Epoch 433/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0314 - accuracy: 0.9396 - val_loss: 0.1534 - val_accuracy: 0.7659\n",
      "Epoch 434/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0310 - accuracy: 0.9388 - val_loss: 0.1516 - val_accuracy: 0.7626\n",
      "Epoch 435/500\n",
      "84/84 [==============================] - 22s 267ms/step - loss: 0.0305 - accuracy: 0.9408 - val_loss: 0.1534 - val_accuracy: 0.7564\n",
      "Epoch 436/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0305 - accuracy: 0.9394 - val_loss: 0.1441 - val_accuracy: 0.7751\n",
      "Epoch 437/500\n",
      "84/84 [==============================] - 21s 252ms/step - loss: 0.0299 - accuracy: 0.9430 - val_loss: 0.1481 - val_accuracy: 0.7722\n",
      "Epoch 438/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0296 - accuracy: 0.9434 - val_loss: 0.1523 - val_accuracy: 0.7632\n",
      "Epoch 439/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0298 - accuracy: 0.9417 - val_loss: 0.1509 - val_accuracy: 0.7630\n",
      "Epoch 440/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0303 - accuracy: 0.9420 - val_loss: 0.1453 - val_accuracy: 0.7666\n",
      "Epoch 441/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0309 - accuracy: 0.9391 - val_loss: 0.1506 - val_accuracy: 0.7595\n",
      "Epoch 442/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0310 - accuracy: 0.9399 - val_loss: 0.1472 - val_accuracy: 0.7609\n",
      "Epoch 443/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0311 - accuracy: 0.9399 - val_loss: 0.1406 - val_accuracy: 0.7730\n",
      "Epoch 444/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0305 - accuracy: 0.9420 - val_loss: 0.1389 - val_accuracy: 0.7747\n",
      "Epoch 445/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0293 - accuracy: 0.9451 - val_loss: 0.1470 - val_accuracy: 0.7624\n",
      "Epoch 446/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0306 - accuracy: 0.9411 - val_loss: 0.1507 - val_accuracy: 0.7639\n",
      "Epoch 447/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.0296 - accuracy: 0.9423 - val_loss: 0.1400 - val_accuracy: 0.7718\n",
      "Epoch 448/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0303 - accuracy: 0.9418 - val_loss: 0.1484 - val_accuracy: 0.7703\n",
      "Epoch 449/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0300 - accuracy: 0.9431 - val_loss: 0.1577 - val_accuracy: 0.7637\n",
      "Epoch 450/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0294 - accuracy: 0.9444 - val_loss: 0.1457 - val_accuracy: 0.7674\n",
      "Epoch 451/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0305 - accuracy: 0.9408 - val_loss: 0.1589 - val_accuracy: 0.7632\n",
      "Epoch 452/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0311 - accuracy: 0.9387 - val_loss: 0.1489 - val_accuracy: 0.7647\n",
      "Epoch 453/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0300 - accuracy: 0.9417 - val_loss: 0.1560 - val_accuracy: 0.7597\n",
      "Epoch 454/500\n",
      "84/84 [==============================] - 22s 257ms/step - loss: 0.0304 - accuracy: 0.9408 - val_loss: 0.1428 - val_accuracy: 0.7722\n",
      "Epoch 455/500\n",
      "84/84 [==============================] - 22s 256ms/step - loss: 0.0295 - accuracy: 0.9441 - val_loss: 0.1507 - val_accuracy: 0.7582\n",
      "Epoch 456/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0293 - accuracy: 0.9435 - val_loss: 0.1420 - val_accuracy: 0.7728\n",
      "Epoch 457/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0296 - accuracy: 0.9428 - val_loss: 0.1433 - val_accuracy: 0.7753\n",
      "Epoch 458/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0300 - accuracy: 0.9430 - val_loss: 0.1426 - val_accuracy: 0.7693\n",
      "Epoch 459/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0303 - accuracy: 0.9409 - val_loss: 0.1531 - val_accuracy: 0.7537\n",
      "Epoch 460/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0308 - accuracy: 0.9394 - val_loss: 0.1520 - val_accuracy: 0.7616\n",
      "Epoch 461/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0305 - accuracy: 0.9397 - val_loss: 0.1500 - val_accuracy: 0.7661\n",
      "Epoch 462/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0292 - accuracy: 0.9431 - val_loss: 0.1481 - val_accuracy: 0.7620\n",
      "Epoch 463/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0290 - accuracy: 0.9442 - val_loss: 0.1587 - val_accuracy: 0.7559\n",
      "Epoch 464/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0298 - accuracy: 0.9434 - val_loss: 0.1567 - val_accuracy: 0.7639\n",
      "Epoch 465/500\n",
      "84/84 [==============================] - 22s 256ms/step - loss: 0.0284 - accuracy: 0.9444 - val_loss: 0.1481 - val_accuracy: 0.7637\n",
      "Epoch 466/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0293 - accuracy: 0.9426 - val_loss: 0.1456 - val_accuracy: 0.7780\n",
      "Epoch 467/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0295 - accuracy: 0.9419 - val_loss: 0.1469 - val_accuracy: 0.7664\n",
      "Epoch 468/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0284 - accuracy: 0.9437 - val_loss: 0.1482 - val_accuracy: 0.7682\n",
      "Epoch 469/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0291 - accuracy: 0.9447 - val_loss: 0.1437 - val_accuracy: 0.7668\n",
      "Epoch 470/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0297 - accuracy: 0.9424 - val_loss: 0.1471 - val_accuracy: 0.7751\n",
      "Epoch 471/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0299 - accuracy: 0.9433 - val_loss: 0.1436 - val_accuracy: 0.7747\n",
      "Epoch 472/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0294 - accuracy: 0.9419 - val_loss: 0.1474 - val_accuracy: 0.7662\n",
      "Epoch 473/500\n",
      "84/84 [==============================] - 22s 256ms/step - loss: 0.0299 - accuracy: 0.9423 - val_loss: 0.1391 - val_accuracy: 0.7760\n",
      "Epoch 474/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0294 - accuracy: 0.9449 - val_loss: 0.1572 - val_accuracy: 0.7614\n",
      "Epoch 475/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0298 - accuracy: 0.9422 - val_loss: 0.1472 - val_accuracy: 0.7714\n",
      "Epoch 476/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0290 - accuracy: 0.9452 - val_loss: 0.1535 - val_accuracy: 0.7659\n",
      "Epoch 477/500\n",
      "84/84 [==============================] - 21s 255ms/step - loss: 0.0294 - accuracy: 0.9443 - val_loss: 0.1555 - val_accuracy: 0.7603\n",
      "Epoch 478/500\n",
      "84/84 [==============================] - 22s 266ms/step - loss: 0.0288 - accuracy: 0.9444 - val_loss: 0.1653 - val_accuracy: 0.7551\n",
      "Epoch 479/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0286 - accuracy: 0.9457 - val_loss: 0.1470 - val_accuracy: 0.7687\n",
      "Epoch 480/500\n",
      "84/84 [==============================] - 22s 256ms/step - loss: 0.0289 - accuracy: 0.9448 - val_loss: 0.1551 - val_accuracy: 0.7586\n",
      "Epoch 481/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0299 - accuracy: 0.9427 - val_loss: 0.1586 - val_accuracy: 0.7453\n",
      "Epoch 482/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0285 - accuracy: 0.9468 - val_loss: 0.1515 - val_accuracy: 0.7747\n",
      "Epoch 483/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0291 - accuracy: 0.9450 - val_loss: 0.1575 - val_accuracy: 0.7691\n",
      "Epoch 484/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0287 - accuracy: 0.9462 - val_loss: 0.1485 - val_accuracy: 0.7703\n",
      "Epoch 485/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0304 - accuracy: 0.9410 - val_loss: 0.1398 - val_accuracy: 0.7720\n",
      "Epoch 486/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0284 - accuracy: 0.9454 - val_loss: 0.1572 - val_accuracy: 0.7584\n",
      "Epoch 487/500\n",
      "84/84 [==============================] - 21s 254ms/step - loss: 0.0282 - accuracy: 0.9469 - val_loss: 0.1495 - val_accuracy: 0.7636\n",
      "Epoch 488/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0288 - accuracy: 0.9434 - val_loss: 0.1471 - val_accuracy: 0.7732\n",
      "Epoch 489/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0288 - accuracy: 0.9451 - val_loss: 0.1564 - val_accuracy: 0.7612\n",
      "Epoch 490/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0289 - accuracy: 0.9437 - val_loss: 0.1530 - val_accuracy: 0.7639\n",
      "Epoch 491/500\n",
      "84/84 [==============================] - 22s 261ms/step - loss: 0.0279 - accuracy: 0.9465 - val_loss: 0.1509 - val_accuracy: 0.7632\n",
      "Epoch 492/500\n",
      "84/84 [==============================] - 22s 264ms/step - loss: 0.0290 - accuracy: 0.9462 - val_loss: 0.1646 - val_accuracy: 0.7395\n",
      "Epoch 493/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0289 - accuracy: 0.9455 - val_loss: 0.1568 - val_accuracy: 0.7599\n",
      "Epoch 494/500\n",
      "84/84 [==============================] - 22s 262ms/step - loss: 0.0282 - accuracy: 0.9465 - val_loss: 0.1496 - val_accuracy: 0.7691\n",
      "Epoch 495/500\n",
      "84/84 [==============================] - 22s 258ms/step - loss: 0.0289 - accuracy: 0.9432 - val_loss: 0.1542 - val_accuracy: 0.7612\n",
      "Epoch 496/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0285 - accuracy: 0.9475 - val_loss: 0.1484 - val_accuracy: 0.7718\n",
      "Epoch 497/500\n",
      "84/84 [==============================] - 22s 259ms/step - loss: 0.0279 - accuracy: 0.9476 - val_loss: 0.1476 - val_accuracy: 0.7712\n",
      "Epoch 498/500\n",
      "84/84 [==============================] - 22s 263ms/step - loss: 0.0287 - accuracy: 0.9449 - val_loss: 0.1480 - val_accuracy: 0.7730\n",
      "Epoch 499/500\n",
      "84/84 [==============================] - 22s 260ms/step - loss: 0.0284 - accuracy: 0.9457 - val_loss: 0.1479 - val_accuracy: 0.7580\n",
      "Epoch 500/500\n",
      "84/84 [==============================] - 22s 265ms/step - loss: 0.0289 - accuracy: 0.9430 - val_loss: 0.1461 - val_accuracy: 0.7653\n",
      "INFO:tensorflow:Assets written to: lstm_cnn.model/assets\n"
     ]
    }
   ],
   "source": [
    "# Defining Constants\n",
    "UNIQUE_CHARACTERS_COUNT = 26\n",
    "MAX_STRING_LENGTH = 452\n",
    "\n",
    "# Preparing the data\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "character_sequences = tokenizer.texts_to_sequences(text_train)\n",
    "sequence_matrix = keras.preprocessing.sequence.pad_sequences(character_sequences)\n",
    "categorical_labels = keras.utils.to_categorical(labels_train)\n",
    "\n",
    "# Splitting the data for having an unseen validation set for the final validation\n",
    "x_train, x_validation, y_train, y_validation = sklearn.model_selection.train_test_split(\n",
    "    sequence_matrix, categorical_labels, test_size=0.20)\n",
    "\n",
    "def deep_learning_based_text_classification(embedding_size=50, dropout_1=0.20, filters=64, kernel_size=5,\n",
    "                                            activation_1 = \"relu\", pool_size=4, lstm_size=50,\n",
    "                                            dense_size_1=200, dense_size_2=12, activation_2 = \"sigmoid\",\n",
    "                                            loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "                                            batch_size=250, epochs=100):\n",
    "    \"\"\"\n",
    "    This function builds an deep learning-based text classification pipeline based on the input parameters.\n",
    "    \"\"\"\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Embedding(UNIQUE_CHARACTERS_COUNT+1, embedding_size, input_length=MAX_STRING_LENGTH))    \n",
    "    model.add(keras.layers.Dropout(dropout_1))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Conv1D(filters, kernel_size, activation=activation_1))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size))\n",
    "    model.add(keras.layers.LSTM(lstm_size))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dense(dense_size_1))\n",
    "    model.add(keras.layers.BatchNormalization())   \n",
    "    model.add(keras.layers.Dense(dense_size_2))\n",
    "    model.add(keras.layers.Activation(activation_2))   \n",
    "    model.summary()\n",
    "    model.compile(loss=loss,optimizer=optimizer,metrics=[\"accuracy\"])\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "    model.save(\"lstm_cnn.model\")\n",
    "\n",
    "\n",
    "deep_learning_based_text_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Evaluation and Prediction\n",
    "Finally, it is time to evaluate our trained model on the validation set that we splitted from the training set. Next, we predict and store the labels of the given unlabeled test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/204 [==============================] - 5s 23ms/step - loss: 0.1439 - accuracy: 0.7626\n",
      "Validation Results:\n",
      "Loss = 0.14\n",
      "Accuracy = 0.76\n",
      "Predicted labels stored.\n"
     ]
    }
   ],
   "source": [
    "# Validating the trained model on the unseen data\n",
    "model = keras.models.load_model(\"lstm_cnn.model\")\n",
    "loss, accuracy = model.evaluate(x_validation, y_validation)\n",
    "print(\"Validation Results:\\nLoss = {:0.2f}\\nAccuracy = {:0.2f}\".format(loss, accuracy))\n",
    "\n",
    "# Predicting labels of the given test set\n",
    "character_sequences = tokenizer.texts_to_sequences(text_test)\n",
    "sequence_matrix = keras.preprocessing.sequence.pad_sequences(character_sequences, maxlen=MAX_STRING_LENGTH)\n",
    "prediction = model.predict(sequence_matrix)\n",
    "f = open(\"ytest.txt\", \"w\")\n",
    "for i in range(len(text_test)):\n",
    "    f.write(str(numpy.argmax(prediction[i])) + \"\\n\")\n",
    "f.close()\n",
    "print(\"Predicted labels stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "We have addressed the obfuscated text classification task. Below, we first summarize the learned lessons and then list the limiatations and the future work.\n",
    "\n",
    "### Summary of Insights\n",
    "- We could not use typical feature extraction approaches, such as tokenization and TF-IDF representation, as our input textual data is obfuscated.\n",
    "- We had to apply a character-level feature representation for the given textual dataset as the text values are continious letters, without any delimiters and spaces to separate tokens.\n",
    "- Traditional feature extraction and classification pipelines did not achive high accuracy because they cannot represent the obfuscated textual data effectively.\n",
    "- Deep neural networks with recurrent layers were promising for the this task as they can learn the language model of the characters.\n",
    "- We could achieve relatively high accuracy for this challenging task using a deep learning model, including LSTM and CNN layers.\n",
    "\n",
    "### Limitations and Future Work\n",
    "- Despite the promises of our solution, the performance could still be improved. There are a lot of design decisions that can change the overall accuracy. Although I have tried my best to optimize them before my deadline, the accuracy is not well-optimized yet as training and testing this deep learning model is time consuming. In particular, our deep learning model could be redesigned by adding or removing some new layers. Furthermore, the hyperparameters can be tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
